{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothing completed. Files saved with 'smoothed_' prefix.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Path to the uploaded zip file\n",
    "output_folder_path =r'H:\\Magdalena\\photometry\\fp_recordings\\ACh_sensory\\sham/new_sham_zscored'\n",
    "\n",
    "# Function to smooth trials\n",
    "def smooth_trials(data_per_animal, trial_types, sigma=20):\n",
    "    smoothed_trials = {}\n",
    "    for animal, trials in data_per_animal.items():\n",
    "        animal_trials = {}\n",
    "        for trial in trial_types:\n",
    "            if trial in trials:\n",
    "                # Apply Gaussian smoothing to each trial\n",
    "                smoothed_data = gaussian_filter1d(trials[trial], sigma=sigma)\n",
    "                animal_trials[trial] = smoothed_data\n",
    "        smoothed_trials[animal] = animal_trials\n",
    "    return smoothed_trials\n",
    "\n",
    "# Process each npy file in all subfolders\n",
    "for root, dirs, files in os.walk(extract_folder_path):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.npy'):\n",
    "            # Load the data\n",
    "            file_path = os.path.join(root, filename)\n",
    "            data = np.load(file_path, allow_pickle=True).item()\n",
    "            \n",
    "            # Get all trial types from the data\n",
    "            trial_types = list(data.keys())\n",
    "            \n",
    "            # Create a dictionary with the original data structure\n",
    "            input_data = {filename: data}\n",
    "            \n",
    "            # Smooth the trials\n",
    "            smoothed_data = smooth_trials(input_data, trial_types)\n",
    "            \n",
    "            # Extract the smoothed data (removing the filename key)\n",
    "            smoothed_dict = smoothed_data[filename]\n",
    "            \n",
    "            # Save the smoothed data\n",
    "            output_filename = f'smoothed_{filename}'\n",
    "            output_path = os.path.join(root, output_filename)\n",
    "            np.save(output_path, smoothed_dict)\n",
    "\n",
    "print(\"Smoothing completed. Files saved with 'smoothed_' prefix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trimmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\files\\data\\sensory_stim\\trim\\sham\\1489_sham.npy\n",
      "Trimmed data saved to: C:\\files\\data\\sensory_stim\\trim\\sham\\1489_sham_trimmed.npy\n",
      "Processing file: C:\\files\\data\\sensory_stim\\trim\\sham\\1490_sham.npy\n",
      "Trimmed data saved to: C:\\files\\data\\sensory_stim\\trim\\sham\\1490_sham_trimmed.npy\n",
      "Processing file: C:\\files\\data\\sensory_stim\\trim\\sham\\1491_sham.npy\n",
      "Trimmed data saved to: C:\\files\\data\\sensory_stim\\trim\\sham\\1491_sham_trimmed.npy\n",
      "Processing file: C:\\files\\data\\sensory_stim\\trim\\sham\\1492_sham.npy\n",
      "Trimmed data saved to: C:\\files\\data\\sensory_stim\\trim\\sham\\1492_sham_trimmed.npy\n",
      "Processing file: C:\\files\\data\\sensory_stim\\trim\\sham_ctrl\\1489_ctrl.npy\n",
      "Trimmed data saved to: C:\\files\\data\\sensory_stim\\trim\\sham_ctrl\\1489_ctrl_trimmed.npy\n",
      "Processing file: C:\\files\\data\\sensory_stim\\trim\\sham_ctrl\\1490_ctrl.npy\n",
      "Trimmed data saved to: C:\\files\\data\\sensory_stim\\trim\\sham_ctrl\\1490_ctrl_trimmed.npy\n",
      "Processing file: C:\\files\\data\\sensory_stim\\trim\\sham_ctrl\\1491_ctrl.npy\n",
      "Trimmed data saved to: C:\\files\\data\\sensory_stim\\trim\\sham_ctrl\\1491_ctrl_trimmed.npy\n",
      "Processing file: C:\\files\\data\\sensory_stim\\trim\\sham_ctrl\\1492_ctrl.npy\n",
      "Trimmed data saved to: C:\\files\\data\\sensory_stim\\trim\\sham_ctrl\\1492_ctrl_trimmed.npy\n",
      "Processing file: C:\\files\\data\\sensory_stim\\trim\\sni\\1497_sni-.npy\n",
      "Trimmed data saved to: C:\\files\\data\\sensory_stim\\trim\\sni\\1497_sni-_trimmed.npy\n",
      "Processing file: C:\\files\\data\\sensory_stim\\trim\\sni\\1498_sni-.npy\n",
      "Trimmed data saved to: C:\\files\\data\\sensory_stim\\trim\\sni\\1498_sni-_trimmed.npy\n",
      "Processing file: C:\\files\\data\\sensory_stim\\trim\\sni\\1499_sni-.npy\n",
      "Trimmed data saved to: C:\\files\\data\\sensory_stim\\trim\\sni\\1499_sni-_trimmed.npy\n",
      "Processing file: C:\\files\\data\\sensory_stim\\trim\\sni\\1500_sni-.npy\n",
      "Trimmed data saved to: C:\\files\\data\\sensory_stim\\trim\\sni\\1500_sni-_trimmed.npy\n",
      "Processing file: C:\\files\\data\\sensory_stim\\trim\\sni_ctrl\\1497_ctrl.npy\n",
      "Trimmed data saved to: C:\\files\\data\\sensory_stim\\trim\\sni_ctrl\\1497_ctrl_trimmed.npy\n",
      "Processing file: C:\\files\\data\\sensory_stim\\trim\\sni_ctrl\\1498_ctrl.npy\n",
      "Trimmed data saved to: C:\\files\\data\\sensory_stim\\trim\\sni_ctrl\\1498_ctrl_trimmed.npy\n",
      "Processing file: C:\\files\\data\\sensory_stim\\trim\\sni_ctrl\\1499_ctrl.npy\n",
      "Trimmed data saved to: C:\\files\\data\\sensory_stim\\trim\\sni_ctrl\\1499_ctrl_trimmed.npy\n",
      "Processing file: C:\\files\\data\\sensory_stim\\trim\\sni_ctrl\\1500_ctrl.npy\n",
      "Trimmed data saved to: C:\\files\\data\\sensory_stim\\trim\\sni_ctrl\\1500_ctrl_trimmed.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the base directory path\n",
    "base_dir = r\"C:\\files\\data\\sensory_stim\\trim\"\n",
    "\n",
    "# Define trimming parameters\n",
    "sample_rate = 130  # Hz\n",
    "trim_start = 5 * sample_rate  # Samples to trim from the start\n",
    "trim_end = 10 * sample_rate  # Samples to trim from the end\n",
    "\n",
    "# Walk through all directories and files in the base directory\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".npy\") and \"_trimmed\" not in file:\n",
    "            # Construct full file path\n",
    "            file_path = os.path.join(root, file)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            \n",
    "            try:\n",
    "                # Load the data with allow_pickle=True\n",
    "                data = np.load(file_path, allow_pickle=True).item()\n",
    "                \n",
    "                # Trim each trace array in the dictionary\n",
    "                for key in data.keys():\n",
    "                    array = data[key]\n",
    "                    # Check if the array has enough samples\n",
    "                    if len(array) < trim_start + trim_end:\n",
    "                        print(f\"Warning: {file_path} - Array for key '{key}' is too short to trim.\")\n",
    "                        continue  # Skip trimming this array\n",
    "                    trimmed_array = array[trim_start:-trim_end]\n",
    "                    data[key] = trimmed_array\n",
    "                \n",
    "                # Create new file path with _trimmed suffix\n",
    "                file_name, ext = os.path.splitext(file)\n",
    "                new_file_name = f\"{file_name}_trimmed{ext}\"\n",
    "                new_file_path = os.path.join(root, new_file_name)\n",
    "                \n",
    "                # Save the trimmed data\n",
    "                np.save(new_file_path, data)\n",
    "                print(f\"Trimmed data saved to: {new_file_path}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z-scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z-scoring completed. Files saved with 'z_scored_' prefix in the corresponding subfolders under 'grabda_zscored'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Path to the uploaded zip file\n",
    "# extract_folder_path = r'H:\\Magdalena\\photometry\\fp_recordings\\ACh_sensory\\sham/'\n",
    "# output_folder_path = r'H:\\Magdalena\\photometry\\fp_recordings\\ACh_sensory\\sham_zscored/'\n",
    "\n",
    "# r'H:\\Magdalena\\photometry\\fp_recordings\\ACh_sensory\\SNI/new_SNI/'\n",
    "# r'H:\\Magdalena\\photometry\\fp_recordings\\ACh_sensory\\sham/new_sham/'\n",
    "\n",
    "extract_folder_path = r'H:\\Magdalena\\photometry\\fp_recordings\\NA_sensory\\new batch\\npy_files/'\n",
    "output_folder_path = r'H:\\Magdalena\\photometry\\fp_recordings\\NA_sensory\\new batch\\npy_files\\new_batch_zscored/'\n",
    "\n",
    "# Function to z-score the data based on the baseline (first 5 seconds)\n",
    "def z_score_trace(trace, baseline_start, baseline_end):\n",
    "    baseline = trace[baseline_start:baseline_end]\n",
    "    baseline_mean = np.mean(baseline)\n",
    "    baseline_std = np.std(baseline)\n",
    "    return (trace - baseline_mean) / baseline_std\n",
    "\n",
    "# Function to z-score trials\n",
    "def z_score_trials(data_per_animal, trial_types, baseline_duration=5, sampling_rate=100):\n",
    "    z_scored_trials = {}\n",
    "    baseline_end = int(baseline_duration * sampling_rate)\n",
    "    \n",
    "    for animal, trials in data_per_animal.items():\n",
    "        animal_trials = {}\n",
    "        for trial in trial_types:\n",
    "            if trial in trials:\n",
    "                # Apply z-scoring to each trial\n",
    "                z_scored_data = z_score_trace(trials[trial], 0, baseline_end)\n",
    "                animal_trials[trial] = z_scored_data\n",
    "        z_scored_trials[animal] = animal_trials\n",
    "    return z_scored_trials\n",
    "\n",
    "# Process each npy file in all subfolders\n",
    "for root, dirs, files in os.walk(extract_folder_path):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.npy'):\n",
    "            # Load the data\n",
    "            file_path = os.path.join(root, filename)\n",
    "            data = np.load(file_path, allow_pickle=True).item()\n",
    "            \n",
    "            # Get all trial types from the data\n",
    "            trial_types = list(data.keys())\n",
    "            \n",
    "            # Create a dictionary with the original data structure\n",
    "            input_data = {filename: data}\n",
    "            \n",
    "            # Z-score the trials\n",
    "            z_scored_data = z_score_trials(input_data, trial_types)\n",
    "            \n",
    "            # Extract the z-scored data (removing the filename key)\n",
    "            z_scored_dict = z_scored_data[filename]\n",
    "            \n",
    "            # Create the corresponding output directory structure\n",
    "            relative_path = os.path.relpath(root, extract_folder_path)\n",
    "            output_dir = os.path.join(output_folder_path, relative_path)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Save the z-scored data\n",
    "            output_filename = f'z_scored_{filename}'\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            np.save(output_path, z_scored_dict)\n",
    "\n",
    "print(\"Z-scoring completed. Files saved with 'z_scored_' prefix in the corresponding subfolders under 'grabda_zscored'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
