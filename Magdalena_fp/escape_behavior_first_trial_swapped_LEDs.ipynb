{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Directory where CSV files are located\n",
    "directory_path = r'H:\\Magdalena\\photometry\\video_analysis_frames\\LDB_NA_Jan_2025'\n",
    "\n",
    "# List of behavior columns in your dataset\n",
    "behavior_columns = ['background', 'light', 'dark'] # can add 'dipintolight' as a behavior later on\n",
    "\n",
    "# Function to process each CSV file\n",
    "def process_csv_file(data_path):\n",
    "    data = pd.read_csv(data_path)\n",
    "    behavior_events = []\n",
    "\n",
    "    for col in behavior_columns:\n",
    "        # Check if the column exists in the DataFrame\n",
    "        if col not in data.columns:\n",
    "            print(f\"Warning: Column '{col}' not found in {data_path}. Skipping this column.\")\n",
    "            continue\n",
    "\n",
    "        onset_indices = data.index[data[col].diff() == 1].tolist()\n",
    "        offset_indices = data.index[data[col].diff() == -1].tolist()\n",
    "\n",
    "        if data[col].iloc[0] == 1:\n",
    "            onset_indices = [0] + onset_indices\n",
    "        if data[col].iloc[-1] == 1:\n",
    "            offset_indices = offset_indices + [data.index[-1]]\n",
    "\n",
    "        for start, end in zip(onset_indices, offset_indices):\n",
    "            behavior_events.append({\n",
    "                'Behavior': col,\n",
    "                'Onset': start,\n",
    "                'Offset': end\n",
    "            })\n",
    "\n",
    "    if behavior_events:  # Check if there are any events to save\n",
    "        behavior_events_df = pd.DataFrame(behavior_events)\n",
    "        base_name = os.path.splitext(data_path)[0]\n",
    "        output_path = f'{base_name}_events.csv'\n",
    "        behavior_events_df.to_csv(output_path, index=False)\n",
    "        print(f'Behavior events saved to {output_path}')\n",
    "    else:\n",
    "        print(f\"No behavior events found in {data_path}. No file created.\")\n",
    "\n",
    "    behavior_events_df = pd.DataFrame(behavior_events)\n",
    "    base_name = os.path.splitext(data_path)[0]\n",
    "    output_path = f'{base_name}_events.csv'\n",
    "    behavior_events_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Search for all CSV files in the directory and its subdirectories\n",
    "csv_files = glob.glob(f'{directory_path}/**/*.csv', recursive=True)\n",
    "\n",
    "# Process each CSV file found\n",
    "# Process each CSV file found, ignoring files that end with '_events.csv'\n",
    "for file_path in csv_files:\n",
    "    if not file_path.endswith('_events.csv'):\n",
    "        process_csv_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average within animal\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.signal import savgol_filter\n",
    "from data_import import import_ppd  # Ensure this function is correctly implemented\n",
    "\n",
    "# Load the LED events Excel file\n",
    "led_events_path = r'H:\\Magdalena\\photometry\\LED_Event_Detection_Results_LDB_NA_Jan_2025.xlsx'\n",
    "led_events_df = pd.read_excel(led_events_path)\n",
    "\n",
    "# Define directories\n",
    "ppd_directory_path = r'H:\\Magdalena\\photometry\\fp_recordings\\LDB\\NA'\n",
    "events_directory_path = r'H:\\Magdalena\\photometry\\video_analysis_frames\\LDB_NA_Jan_2025'\n",
    "\n",
    "pre_start = 5  # Seconds before the event\n",
    "post_start = 15  # Seconds after the event\n",
    "\n",
    "def process_behavior_trace(dFF, behavior_array, sampling_rate, ax, behavior_name):\n",
    "    \"\"\"\n",
    "    Processes trace data for a specific behavior and plots the average trace.\n",
    "    \"\"\"\n",
    "    num_points_per_trace = int((pre_start + post_start) * sampling_rate)\n",
    "    trace_data_matrix = []\n",
    "    \n",
    "    for start_time in behavior_array[:, 0]:\n",
    "        start = int(start_time - pre_start * sampling_rate)\n",
    "        end = int(start + num_points_per_trace)\n",
    "        if start < 0 or end > len(dFF):  # Skip if trace goes out of bounds\n",
    "            continue\n",
    "        trace_data = dFF[start:end]\n",
    "        baseline_start = int(start_time - 5 * sampling_rate)\n",
    "        baseline_end = int(start_time - 3 * sampling_rate)\n",
    "        baseline = np.mean(dFF[baseline_start:baseline_end])\n",
    "        relative_trace_data = trace_data - baseline\n",
    "        trace_data_matrix.append(relative_trace_data)\n",
    "    \n",
    "    if trace_data_matrix:\n",
    "        trace_data_matrix = np.array(trace_data_matrix)\n",
    "        average_trace = np.mean(trace_data_matrix, axis=0)\n",
    "        time = np.arange(-pre_start, post_start, 1 / sampling_rate)\n",
    "        \n",
    "        # Plot average trace for the behavior on the shared axis\n",
    "        ax.plot(time, average_trace, label=behavior_name)\n",
    "        plt.axvline(x=0, color='black', linestyle='--')  # Add y=0 dashed line\n",
    "\n",
    "\n",
    "def process_ppd_file(ppd_file_path, led_events_df, events_directory_path):\n",
    "    filename_base = os.path.splitext(os.path.basename(ppd_file_path))[0]\n",
    "    data = import_ppd(ppd_file_path, low_pass=20, high_pass=0.001)\n",
    "    sampling_rate = data['sampling_rate']\n",
    "    print(sampling_rate)\n",
    "    # Convert sample index to time vector\n",
    "    time = np.arange(len(data['analog_1'])) / sampling_rate\n",
    "    \n",
    "    # dFF calculation                                                # swapping analog_1 and analog_2 because in the escape behavior recording on 20250317 and 20250318 I had the LEDs plugged in the wrong way (swapped)\n",
    "    reg = np.polyfit(data['analog_1'], data['analog_2'], 1)\n",
    "    fit_405 = reg[0] * data['analog_1'] + reg[1]\n",
    "    dFF = (data['analog_2'] - fit_405) / fit_405\n",
    "\n",
    "    \"\"\"     # Save analog_1, analog_2, dFF to CSV\n",
    "    save_data_path = os.path.join(os.path.dirname(ppd_file_path), filename_base + '_data.csv')\n",
    "    df_to_save = pd.DataFrame({\n",
    "        'Time': time,\n",
    "        'Analog_1': data['analog_1'],\n",
    "        'Analog_2': data['analog_2'],\n",
    "        'dFF': dFF\n",
    "    })\n",
    "    df_to_save.to_csv(save_data_path, index=False)\n",
    "    print(f\"Data saved to: {save_data_path}\") \"\"\"\n",
    "    \n",
    "    # Create figure and subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Plot analog signals and fit\n",
    "    ax1.plot(time, data['analog_1'], label='analog_1')\n",
    "    ax1.plot(time, data['analog_2'], label='analog_2')\n",
    "    ax1.plot(time, fit_405, label='fit_405')\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Value')\n",
    "    ax1.set_title('Analog Signals and Fit')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot dFF\n",
    "    ax2.plot(time, dFF, label='dFF')\n",
    "    ax2.set_xlabel('Time')\n",
    "    ax2.set_ylabel('dFF')\n",
    "    ax2.set_title('dFF')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.suptitle(filename_base)\n",
    "    \n",
    "    # Save figure\n",
    "    save_path = os.path.join(os.path.dirname(ppd_file_path), filename_base + '.png')\n",
    "    fig.savefig(save_path, dpi=300)\n",
    "    plt.close()  # Close the plot to save memory\n",
    "\n",
    "    print(f\"Processed and saved: {save_path}\")\n",
    "\n",
    "    row = led_events_df[led_events_df.iloc[:, 0] == filename_base]\n",
    "    if row.empty:\n",
    "        print(f\"{filename_base} not found in LED event detection results.\")\n",
    "        return\n",
    "\n",
    "    led_on = row['Onset_Point'].values[0]\n",
    "    led_off = row['Offset_Point'].values[0]\n",
    "    fps = row['fps'].values[0]\n",
    "\n",
    "    events_files = glob.glob(f\"{events_directory_path}/**/{filename_base}_events.csv\", recursive=True)\n",
    "    if not events_files:\n",
    "        print(f\"No matching events file found for {filename_base}.\")\n",
    "        return\n",
    "\n",
    "    for events_file in events_files:\n",
    "        events_data = pd.read_csv(events_file)\n",
    "        events_data['Onset'] = (events_data['Onset'] - led_on) / fps * sampling_rate\n",
    "        events_data['Offset'] = (events_data['Offset'] - led_on) / fps * sampling_rate\n",
    "\n",
    "        unique_behaviors = events_data['Behavior'].unique()\n",
    "        #unique_behaviors = ['background', 'light', 'dark', 'dipintolight']\n",
    "        selected_behaviors = ['dark' 'light']\n",
    "        behavior_arrays = {}\n",
    "        for behavior in unique_behaviors:\n",
    "            filtered_events = events_data[events_data['Behavior'] == behavior]\n",
    "            behavior_array = np.column_stack((filtered_events['Onset'].values, filtered_events['Offset'].values))\n",
    "            behavior_arrays[behavior] = behavior_array\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))  # Shared plot for all behaviors\n",
    "    \n",
    "    # Process and plot each behavior on the same axes\n",
    "    for behavior, behavior_array in behavior_arrays.items():\n",
    "        process_behavior_trace(dFF, behavior_array, sampling_rate, ax, behavior)\n",
    "    \n",
    "    # Finalizing the plot\n",
    "    ax.set_xlabel('Time (seconds)')\n",
    "    ax.set_ylabel('Average Relative dFF')\n",
    "    ax.set_title(f'Average Traces for {filename_base}')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Processed and plotted average traces for: {filename_base}\")\n",
    "\n",
    "\n",
    "\n",
    "# Process each .ppd file found in the directory\n",
    "for ppd_file_path in glob.glob(os.path.join(ppd_directory_path, '**', '*.ppd'), recursive=True):\n",
    "    process_ppd_file(ppd_file_path, led_events_df, events_directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.signal import resample\n",
    "from numpy.typing import NDArray\n",
    "from data_import import import_ppd  # Ensure this function is correctly implemented\n",
    "\n",
    "# Load the LED events Excel file\n",
    "led_events_path = r'H:\\Magdalena\\photometry\\LED_Event_Detection_Results_LDB_NA_Jan_2025.xlsx'\n",
    "led_events_df = pd.read_excel(led_events_path)\n",
    "\n",
    "sampling_rate = 100\n",
    "target_sampling_rate = 100  # Hz\n",
    "pre_start = 5  # Seconds before the event\n",
    "post_start = 15  # Seconds after the event\n",
    "behavior_trace_data = {}  # Dictionary to store behavior trace data\n",
    "\n",
    "\n",
    "def process_behavior_trace(dFF, behavior_array, sampling_rate, behavior_name):\n",
    "    \"\"\"\n",
    "    Processes trace data for a specific behavior and stores individual traces.\n",
    "    \"\"\"\n",
    "    num_points_per_trace = int((pre_start + post_start) * sampling_rate)\n",
    "    \n",
    "    for start_time in behavior_array[:, 0]:\n",
    "        start = int(start_time - pre_start * sampling_rate)\n",
    "        end = int(start + num_points_per_trace)\n",
    "        if start < 0 or end > len(dFF):  # Skip if trace goes out of bounds\n",
    "            continue\n",
    "        trace_data = dFF[start:end]\n",
    "        baseline_start = int(start_time - 5 * sampling_rate)\n",
    "        baseline_end = int(start_time - 3 * sampling_rate)\n",
    "        baseline = np.mean(dFF[baseline_start:baseline_end])\n",
    "        relative_trace_data = trace_data - baseline\n",
    "        \n",
    "        # Store individual traces in the behavior_trace_data dictionary\n",
    "        if behavior_name not in behavior_trace_data:\n",
    "            behavior_trace_data[behavior_name] = [relative_trace_data]\n",
    "        else:\n",
    "            behavior_trace_data[behavior_name].append(relative_trace_data)\n",
    "\n",
    "\n",
    "def process_group(ppd_directory_path, events_directory_path):\n",
    "    global behavior_trace_data\n",
    "    behavior_trace_data = {}  # Reset for each group\n",
    "\n",
    "    # Process each .ppd file in the directory for the current group\n",
    "    for ppd_file_path in glob.glob(os.path.join(ppd_directory_path, '**', '*.ppd'), recursive=True):\n",
    "        process_ppd_file(ppd_file_path, led_events_df, events_directory_path)\n",
    "    \n",
    "    # Prepare data for plotting with trial numbers\n",
    "    group_data = {}\n",
    "    for behavior, individual_traces in behavior_trace_data.items():\n",
    "        pooled_traces = np.stack(individual_traces, axis=0)\n",
    "        average_trace = np.mean(pooled_traces, axis=0)\n",
    "        sem = np.std(pooled_traces, axis=0) / np.sqrt(pooled_traces.shape[0])\n",
    "        smoothed_average_trace = savgol_filter(average_trace, window_length=11, polyorder=2)\n",
    "        smoothed_sem = savgol_filter(sem, window_length=11, polyorder=2)\n",
    "        num_trials = len(individual_traces)  # Count the number of trials\n",
    "        group_data[behavior] = (smoothed_average_trace, smoothed_sem, num_trials)\n",
    "    return group_data\n",
    "\n",
    "\n",
    "\n",
    "def process_ppd_file(ppd_file_path, led_events_df, events_directory_path):\n",
    "    filename_base = os.path.splitext(os.path.basename(ppd_file_path))[0]\n",
    "    data = import_ppd(ppd_file_path, low_pass=20, high_pass=0.001)\n",
    "    sampling_rate = data['sampling_rate']\n",
    "    # Convert sample index to time vector\n",
    "    time = np.arange(len(data['analog_1'])) / sampling_rate\n",
    "    \n",
    "    # dFF calculation\n",
    "    reg = np.polyfit(data['analog_2'], data['analog_1'], 1)\n",
    "    fit_405 = reg[0] * data['analog_2'] + reg[1]\n",
    "    dFF = (data['analog_1'] - fit_405) / fit_405\n",
    "\n",
    "    \"\"\"     # Save analog_1, analog_2, dFF to CSV\n",
    "    save_data_path = os.path.join(os.path.dirname(ppd_file_path), filename_base + '_data.csv')\n",
    "    df_to_save = pd.DataFrame({\n",
    "        'Time': time,\n",
    "        'Analog_1': data['analog_1'],\n",
    "        'Analog_2': data['analog_2'],\n",
    "        'dFF': dFF\n",
    "    })\n",
    "    df_to_save.to_csv(save_data_path, index=False)\n",
    "    print(f\"Data saved to: {save_data_path}\") \"\"\"\n",
    "    \n",
    "    # Create figure and subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Plot analog signals and fit\n",
    "    ax1.plot(time, data['analog_1'], label='analog_1')\n",
    "    ax1.plot(time, data['analog_2'], label='analog_2')\n",
    "    ax1.plot(time, fit_405, label='fit_405')\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Value')\n",
    "    ax1.set_title('Analog Signals and Fit')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot dFF\n",
    "    ax2.plot(time, dFF, label='dFF')\n",
    "    ax2.set_xlabel('Time')\n",
    "    ax2.set_ylabel('dFF')\n",
    "    ax2.set_title('dFF')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.suptitle(filename_base)\n",
    "    \n",
    "    # Save figure\n",
    "    save_path = os.path.join(os.path.dirname(ppd_file_path), filename_base + '.png')\n",
    "    fig.savefig(save_path, dpi=300)\n",
    "    plt.close()  # Close the plot to save memory\n",
    "\n",
    "    print(f\"Processed and saved: {save_path}\")\n",
    "\n",
    "    # Index of np.diff(data['digital_1']) bigger than 0.5 or smaller than -0.5\n",
    "    index = np.where((np.diff(data['digital_1']) > 0.5) | (np.diff(data['digital_1']) < -0.5))\n",
    "\n",
    "    # Print the onsets and offsets\n",
    "    print(\"index:\", index)\n",
    "\n",
    "    dFF = dFF[index[0][0]:]\n",
    "\n",
    "    if sampling_rate != 100:\n",
    "        # Calculate the total duration of the signal in seconds\n",
    "        duration = len(dFF) / sampling_rate  # Number of samples divided by the original sampling rate\n",
    "\n",
    "        # Calculate the number of samples in the resampled signal\n",
    "        num_samples_target = int(duration * target_sampling_rate)  # Duration times target sampling rate\n",
    "\n",
    "        # Resample the data\n",
    "        dFF = resample(dFF, num_samples_target)\n",
    "\n",
    "        # Generate the new time vector for the resampled data\n",
    "        time = np.linspace(0, duration, num_samples_target)\n",
    "\n",
    "    row = led_events_df[led_events_df.iloc[:, 0] == filename_base]\n",
    "    if row.empty:\n",
    "        print(f\"{filename_base} not found in LED event detection results.\")\n",
    "        return\n",
    "\n",
    "    led_on = row['Onset_Point'].values[0]\n",
    "    led_off = row['Offset_Point'].values[0]\n",
    "    fps = row['fps'].values[0]\n",
    "\n",
    "    events_files = glob.glob(f\"{events_directory_path}/**/{filename_base}_events.csv\", recursive=True)\n",
    "    if not events_files:\n",
    "        print(f\"No matching events file found for {filename_base}.\")\n",
    "        return\n",
    "\n",
    "    for events_file in events_files:\n",
    "        events_data = pd.read_csv(events_file)\n",
    "        events_data['Onset'] = (events_data['Onset'] - led_on) / fps * target_sampling_rate\n",
    "        events_data['Offset'] = (events_data['Offset'] - led_on) / fps * target_sampling_rate\n",
    "\n",
    "        unique_behaviors = events_data['Behavior'].unique()\n",
    "        #unique_behaviors = ['background' 'approach' 'general' 'nose-nose' 'nose-tail' 'fight']\n",
    "        selected_behaviors = ['background' 'nose-nose']\n",
    "        behavior_arrays = {}\n",
    "        for behavior in unique_behaviors:\n",
    "            filtered_events = events_data[events_data['Behavior'] == behavior]\n",
    "            behavior_array = np.column_stack((filtered_events['Onset'].values, filtered_events['Offset'].values))\n",
    "            behavior_arrays[behavior] = behavior_array\n",
    "    \n",
    "    # Process and plot each behavior on the same axes\n",
    "    for behavior, behavior_array in behavior_arrays.items():\n",
    "        process_behavior_trace(dFF, behavior_array, target_sampling_rate, behavior)\n",
    "\n",
    "\n",
    "\n",
    "ppd_directory_path = r'H:\\Magdalena\\photometry\\fp_recordings\\LDB\\NA\\mouse_1'\n",
    "events_directory_path = r'H:\\Magdalena\\photometry\\video_analysis_frames\\LDB_NA_Jan_2025'\n",
    "\n",
    "# Define paths for Sham and SNI groups\n",
    "sham_paths = {\n",
    "    'ppd_directory_path': r'H:\\Magdalena\\photometry\\fp_recordings\\LDB\\NA\\sham',\n",
    "    'events_directory_path': r'H:\\Magdalena\\photometry\\video_analysis_frames\\LDB_NA_Jan_2025\\sham'\n",
    "}\n",
    "sni_paths = {\n",
    "    'ppd_directory_path': r'H:\\Magdalena\\photometry\\fp_recordings\\LDB\\NA\\SNI',\n",
    "    'events_directory_path': r'H:\\Magdalena\\photometry\\video_analysis_frames\\LDB_NA_Jan_2025\\SNI'\n",
    "}\n",
    "\n",
    "# Process data for both groups\n",
    "sham_data = process_group(**sham_paths)\n",
    "sni_data = process_group(**sni_paths)\n",
    "\n",
    "# Determine all behaviors to plot\n",
    "all_behaviors = set(sham_data.keys()).union(sni_data.keys())\n",
    "\n",
    "# Plot data for each behavior with trial numbers in the legend\n",
    "for behavior in all_behaviors:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    time = np.arange(-pre_start, post_start, 1 / sampling_rate)\n",
    "    \n",
    "    # Plot Sham data if available\n",
    "    if behavior in sham_data:\n",
    "        avg_trace, sem, num_trials_sham = sham_data[behavior]\n",
    "        plt.plot(time, avg_trace, label=f'Sham - {behavior} (N={num_trials_sham})', color='blue')\n",
    "        plt.fill_between(time, avg_trace - sem, avg_trace + sem, color='blue', alpha=0.2)\n",
    "    \n",
    "    # Plot SNI data if available\n",
    "    if behavior in sni_data:\n",
    "        avg_trace, sem, num_trials_sni = sni_data[behavior]\n",
    "        plt.plot(time, avg_trace, label=f'SNI - {behavior} (N={num_trials_sni})', color='red')\n",
    "        plt.fill_between(time, avg_trace - sem, avg_trace + sem, color='red', alpha=0.2)\n",
    "    \n",
    "    plt.axvline(x=0, color='k', linestyle='--')\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('dFF (%)')\n",
    "    plt.ylim(-0.002, 0.003)\n",
    "    plt.title(f'{behavior} - GRAB-NA-LDB Sham vs. SNI')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
