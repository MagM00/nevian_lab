{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photometry data preprocessing\n",
    "\n",
    "This notebook shows methods for preprocessing fiber photometry data. The preprocessing consists of the following steps:\n",
    "\n",
    "1. Lowpass and median filtering to reduce noise and electrical artifacts.\n",
    "\n",
    "2. Correction for photobleaching, i.e. the slow decreace in the fluorescence signal as flurophores are degraded by light exposure.  Three different methods are shown (i) highpass filtering with a very low cutoff frequency, (ii) Subtraction of a exponential fit (ii) Subtraction of a polynomial fit.\n",
    "\n",
    "2. Movement correction.  Movement artifacts are estimated by a linear fit of the movement control channel to the GCaMP signal, then subtracted from the GCaMP signal.\n",
    "\n",
    "4. Conversion of the signal to dF/F.\n",
    "\n",
    "Note that different groups do preprocessing differently and there is no universally accepted best practice for how to preprocess photometry data.  The best way to preprocess your data will likely depend on the details of the experimental setup and the questions you want to ask of the data. It is good practice to always visually inspect the raw data and the results of each preprocessing step to make sure they look sensible.  \n",
    "\n",
    "The data used in this notebook were recorded from dopamine neurons in mouse VTA during a reward guided decision task.  The data were recorded using [pyPhotometry](https://pyphotometry.readthedocs.io) acquisition hardware, using the *two colour time division* acquisition mode with GCaMP6f as the calcium indicator and TdTomato as a movement control channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the standard python modules needed for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as  np\n",
    "import pylab as plt\n",
    "from scipy.signal import medfilt, butter, filtfilt\n",
    "from scipy.stats import linregress\n",
    "from scipy.optimize import curve_fit, minimize\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [14, 12] # Make default figure size larger.\n",
    "plt.rcParams['axes.xmargin'] = 0          # Make default margin on x axis zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the pyPhotometry [data import](https://pyphotometry.readthedocs.io/en/latest/user-guide/importing-data/) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_import import import_ppd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'H:\\fp_test\\pyBoard'\n",
    "data_filename = '517-2023-06-30-152326.ppd'\n",
    "data = import_ppd(os.path.join(data_folder, data_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the raw GCaMP and TdTomato signals, session time, and sampling rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_raw = data['analog_1']\n",
    "violet_raw = data['analog_2']\n",
    "time_seconds = data['time']/1000\n",
    "sampling_rate = data['sampling_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw signals\n",
    "\n",
    "Let's take a look at the raw GCaMP and TdTomato signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time_seconds, sensor_raw, 'g', label='GCaMP')\n",
    "plt.plot(time_seconds, violet_raw, 'r', label='TdTomato')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Signal (volts)')\n",
    "plt.title('Raw signals')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising\n",
    "\n",
    "This recording has large electrical noise artifacts, likely due to the high gain amplifiers in the photodetectors picking up signals from a nearby mobile phone.  The artifacts are very short pulses and can be greatly reduced by running a median filter before the standard low pass filter.  We then lowpass filter the signals to reduce noise, using a zero phase filter with a 10Hz cutoff frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median filtering to remove electrical artifact.\n",
    "GCaMP_denoised = medfilt(sensor_raw, kernel_size=5)\n",
    "TdTom_denoised = medfilt(violet_raw, kernel_size=5)\n",
    " \n",
    "# Lowpass filter - zero phase filtering (with filtfilt) is used to avoid distorting the signal.\n",
    "b,a = butter(2, 10, btype='low', fs=sampling_rate)\n",
    "GCaMP_denoised = filtfilt(b,a, GCaMP_denoised)\n",
    "TdTom_denoised = filtfilt(b,a, TdTom_denoised)\n",
    "\n",
    "#plt.plot(time_seconds, GCaMP_raw, label='GCaMP')\n",
    "#plt.plot(time_seconds, TdTom_raw, label='TdTomato')\n",
    "plt.plot(time_seconds, GCaMP_denoised, 'g', label='GCaMP denoised')\n",
    "plt.plot(time_seconds, TdTom_denoised, 'r', label='TdTom denoised') \n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Signal (volts)')\n",
    "plt.title('Denoised signals')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's zoom in on the x axis to see how the lowpass filtering has smoothed the signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time_seconds, GCaMP_raw, label='GCaMP raw')\n",
    "plt.plot(time_seconds, TdTom_raw, label='TdTomato raw')\n",
    "plt.plot(time_seconds, GCaMP_denoised, label='GCaMP denoised')\n",
    "plt.plot(time_seconds, TdTom_denoised, label='TdTom denoised') \n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Signal (volts)')\n",
    "plt.title('Denoised signals')\n",
    "plt.legend()\n",
    "plt.xlim(0,60)\n",
    "plt.ylim(1,1.4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photobleaching correction\n",
    "\n",
    "Now lets compare some different ways of removing the influence of photobleaching.\n",
    "\n",
    "A simple way to remove slow changes is simply to highpass filter the signal with a very low cutoff frequency. Here we will high pass at 0.001Hz, which correponds to a period of 16 minutes.  All components of the signal changing on a slower timescale than this will be removed, which removes the drift due to bleaching, but will also remove any physiological variation in the signal on very slow timescales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "b,a = butter(2, 0.001, btype='high', fs=sampling_rate)\n",
    "GCaMP_highpass = filtfilt(b,a, GCaMP_denoised, padtype='even')\n",
    "TdTom_highpass = filtfilt(b,a, TdTom_denoised, padtype='even')\n",
    "\n",
    "plt.plot(time_seconds, GCaMP_highpass    ,'g', label='GCaMP highpass')\n",
    "plt.plot(time_seconds, TdTom_highpass-0.1,'r', label='TdTom highpass')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Signal (volts)')\n",
    "plt.title('Bleaching correction by highpass filtering')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annother way of removing the influence of bleaching is to fit an exponential decay to the data and subtract this exponential fit from the signal. In practice we find that a double exponential fit is preferable because there are typically multiple sources of fluorescence that contribute to the bleaching (e.g. autofluorescence from fiber, autofluorescence from brain tissue, and flurophore fluorescence), which may bleach at different rates, so a single exponential fit can be overly restrictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The double exponential curve we are going to fit.\n",
    "def double_exponential(t, const, amp_fast, amp_slow, tau_slow, tau_multiplier):\n",
    "    '''Compute a double exponential function with constant offset.\n",
    "    Parameters:\n",
    "    t       : Time vector in seconds.\n",
    "    const   : Amplitude of the constant offset. \n",
    "    amp_fast: Amplitude of the fast component.  \n",
    "    amp_slow: Amplitude of the slow component.  \n",
    "    tau_slow: Time constant of slow component in seconds.\n",
    "    tau_multiplier: Time constant of fast component relative to slow. \n",
    "    '''\n",
    "    tau_fast = tau_slow*tau_multiplier\n",
    "    return const+amp_slow*np.exp(-t/tau_slow)+amp_fast*np.exp(-t/tau_fast)\n",
    "\n",
    "# Fit curve to GCaMP signal.\n",
    "max_sig = np.max(GCaMP_denoised)\n",
    "inital_params = [max_sig/2, max_sig/4, max_sig/4, 3600, 0.1]\n",
    "bounds = ([0      , 0      , 0      , 600  , 0],\n",
    "          [max_sig, max_sig, max_sig, 36000, 1])\n",
    "GCaMP_parms, parm_cov = curve_fit(double_exponential, time_seconds, GCaMP_denoised, \n",
    "                                  p0=inital_params, bounds=bounds, maxfev=1000)\n",
    "GCaMP_expfit = double_exponential(time_seconds, *GCaMP_parms)\n",
    "\n",
    "# Fit curve to TdTomato signal.\n",
    "max_sig = np.max(TdTom_denoised)\n",
    "inital_params = [max_sig/2, max_sig/4, max_sig/4, 3600, 0.1]\n",
    "bounds = ([0      , 0      , 0      , 600  , 0],\n",
    "          [max_sig, max_sig, max_sig, 36000, 1])\n",
    "TdTom_parms, parm_cov = curve_fit(double_exponential, time_seconds, TdTom_denoised, \n",
    "                                  p0=inital_params, bounds=bounds, maxfev=1000)\n",
    "TdTom_expfit = double_exponential(time_seconds, *TdTom_parms)\n",
    "\n",
    "plt.plot(time_seconds, GCaMP_denoised, 'g', label='GCaMP')\n",
    "plt.plot(time_seconds, GCaMP_expfit,'k', linewidth=1.5) \n",
    "plt.plot(time_seconds, TdTom_denoised, 'r', label='TdTomato')\n",
    "plt.plot(time_seconds, TdTom_expfit,'k', linewidth=1.5) \n",
    "plt.title('Exponential fit to bleaching.')\n",
    "plt.xlabel('Time (seconds)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now subtract the exponential fits from the signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCaMP_detrended = GCaMP_denoised - GCaMP_expfit\n",
    "TdTom_detrended = TdTom_denoised - TdTom_expfit\n",
    "\n",
    "plt.plot(time_seconds, GCaMP_detrended    , 'g', label='GCaMP')\n",
    "plt.plot(time_seconds, TdTom_detrended-0.1, 'r', label='TdTomato')\n",
    "plt.title('Bleaching correction by subtraction of exponential fit')\n",
    "plt.xlabel('Time (seconds)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion correction\n",
    "\n",
    "We now do motion correction by finding the best linear fit of the TdTomato signal to the GCaMP signal and subtracting this estimated motion component from the GCaMP signal.  We will use the data that was bleaching corrected using the double exponential fit as we find that in practice this is less likely to remove meaningful slow variation in the signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = linregress(x=TdTom_detrended, y=GCaMP_detrended)\n",
    "\n",
    "plt.scatter(TdTom_detrended[::5], GCaMP_detrended[::5],alpha=0.1, marker='.')\n",
    "x = np.array(plt.xlim())\n",
    "plt.plot(x, intercept+slope*x)\n",
    "plt.xlabel('TdTomato')\n",
    "plt.ylabel('GCaMP')\n",
    "plt.title('TdTomato - GCaMP correlation.')\n",
    "\n",
    "print('Slope    : {:.3f}'.format(slope))\n",
    "print('R-squared: {:.3f}'.format(r_value**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the estimated motion component of the GCaMP signal and subtract to get motion corrected signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCaMP_est_motion = intercept + slope * TdTom_detrended\n",
    "GCaMP_corrected = GCaMP_detrended - GCaMP_est_motion\n",
    "\n",
    "plt.plot(time_seconds, GCaMP_detrended  , label='GCaMP - pre motion correction')\n",
    "plt.plot(time_seconds, GCaMP_corrected, 'g', label='GCaMP - motion corrected')\n",
    "plt.plot(time_seconds, GCaMP_est_motion - 0.05, 'y', label='estimated motion')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.title('Motion correction')\n",
    "plt.legend()\n",
    "plt.xlim(0,180);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation\n",
    "\n",
    "Typically in a photometry experiment we want to combine data across sessions and/or subjects.  This is complicated by the fact that different sessions may have different levels of fluorphores expression, excitation light and autofluorescence.  It is therefore desirable to normalise the data to reduce this variability.  The two most widely used ways of doing this are computing dF/F or z-scores.\n",
    "\n",
    "## dF/F\n",
    "\n",
    "To compute dF/F we divide the signal changes (dF) by the baseline fluorescence (F) and multiply by 100 to convert to percent. The dF is just the motion corrected signal plotted above.  The baseline fluorescence F changes over the course of the session due to photobleaching, and is just the baseline we estimated with our double exponential fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCaMP_dF_F = 100*GCaMP_corrected/GCaMP_expfit\n",
    "\n",
    "plt.plot(time_seconds, GCaMP_dF_F, 'g')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('GCaMP dF/F (%)')\n",
    "plt.title('GCaMP dF/F')\n",
    "plt.xlim(0,180);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-scoring\n",
    "\n",
    "Alternatively, we can normalise the data by z-scoring each session - i.e. subtracting the mean and dividing by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCaMP_zscored = (GCaMP_corrected-np.mean(GCaMP_corrected))/np.std(GCaMP_corrected)\n",
    "\n",
    "plt.plot(time_seconds, GCaMP_zscored, 'g')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('GCaMP (z-scored)')\n",
    "plt.title('GCaMP z-scored')\n",
    "plt.xlim(0,180);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Copyright Thomas Akam 2019 - 2023.  Released under the [GPL3 Licence](https://www.gnu.org/licenses/gpl-3.0.en.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
